# MCP Code Crosscheck

ðŸš§ **UNDER CONSTRUCTION** ðŸš§

A POC MCP (Model Context Protocol) server that attempts to reduce bias in AI code review by requesting different models for code generation and review, based on recent research suggesting self-evaluation bias in language models.

## Overview

This server implements an experimental approach based on the hypothesis that when the same model generates and evaluates code, it may exhibit self-preference bias, potentially leading to blind spots in code quality assessment.

**Approach**: Cross-model code review - when code is generated by one model, request a different model family for review. The effectiveness of this approach is still being studied, and results may vary depending on the specific models and code being reviewed.

## Research Context

Recent studies suggest potential bias in AI self-evaluation, though the practical implications for code review are still being explored:

### Self-Recognition and Self-Preference
- **[LLM Evaluators Recognize and Favor Their Own Generations](https://arxiv.org/abs/2404.13076)** (Panickssery et al., UC San Diego, April 2024)
  - Found evidence of self-recognition leading to self-preference bias in controlled settings
  - Models showed tendency to rate their own outputs higher when identifiable
  - Discovered linear correlation between self-recognition capability and bias strength

### Self-Bias Across Different Models
- **[Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement](https://arxiv.org/abs/2402.11436)** (Xu et al., February 2024)
  - Observed self-bias patterns in 6 major LLMs in specific evaluation tasks
  - Suggests self-refinement may amplify rather than reduce bias in some contexts

### Code-Specific Evaluation Bias
- **[Don't Judge Code by Its Cover: Exploring Biases in LLM Judges for Code Evaluation](https://arxiv.org/abs/2505.16222)** (Moon et al., May 2025)
  - First comprehensive study of LLM bias specifically in code evaluation
  - Identified 6 distinct bias types: formatting, naming, comments, language-specific patterns
  - Found biases persist even with test-case generation strategies
  - Validates need for cross-model evaluation in code review contexts

### Quantifying LLM-as-Judge Biases
- **[Justice or Prejudice? Quantifying Biases in LLM-as-a-Judge](https://arxiv.org/abs/2410.02736)** (Li et al., October 2024)
  - Comprehensive benchmark showing 40% average bias rate across models
  - Demonstrates effectiveness of multi-model evaluation approaches
  - Provides quantitative evidence for bias reduction through model diversity

- **[Self-Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2410.21819)** (Wu et al., October 2024)
  - Shows 50%+ of evaluations exhibit favorable bias toward own outputs
  - Demonstrates bias persistence even with anonymized sources
  - Supports using different models for generation vs evaluation

### Multi-Agent Code Generation Success
- **[MapCoder: Multi-Agent Code Generation for Competitive Problem Solving](https://aclanthology.org/2024.acl-long.269/)** (Islam et al., ACL 2024)
  - Achieved 93.9% pass@1 on HumanEval using separated agents
  - Demonstrates effectiveness of role separation in reducing bias
  - Validates multi-agent architecture for code tasks

### Self-Recognition Mechanisms
- **[Self-Recognition in Language Models](https://arxiv.org/abs/2407.06946)** (Wataoka et al., July 2024)
  - Explains "familiarity hypothesis": models prefer outputs matching their training patterns
  - Shows models evaluate based on perplexity alignment
  - Provides theoretical foundation for cross-model evaluation

### Structured Prompting Effects
- **[Principled Instructions Are All You Need](https://arxiv.org/abs/2312.16171)** (VILA Lab, December 2023)
  - Tested prompting approaches across different models and tasks
  - Found structured instructions can improve performance, though results vary by task
  - Emotional framing showed modest improvements in some scenarios

### Multi-Agent Evaluation
- **[Two Tales of Persona in LLMs](https://arxiv.org/html/2406.01171v1)** (June 2024)
  - Explored role-based prompting in multi-agent systems
  - Suggests adversarial personas may improve evaluation in certain contexts

**Note**: While these studies provide motivation for cross-model evaluation, their applicability to code review specifically has not been extensively validated. The effectiveness of this approach may vary significantly based on the models used, code complexity, and review criteria.

## Features

### Tools

- **`review_code`**: Review code with bias mitigation using cross-model evaluation. Requires a detected generation model.
- **`detect_model_from_authors`**: Detect AI model from commit author information. Use this with data from GitHub sources.
- **`fetch_commit`**: Fetch commit details using GitHub CLI. Fallback tool - prefer GitHub MCP server if available.
- **`fetch_pr_commits`**: Fetch PR commits using GitHub CLI. Fallback tool - prefer GitHub MCP server if available.

### Prompts

- **`code_review`**: Comprehensive code review covering security, performance, and maintainability with structured checklist output

## How It Works

### Modular Architecture

This server uses a modular design that allows seamless integration with GitHub MCP servers when available, while providing fallback functionality through GitHub CLI.

**Server Instructions**: The server provides comprehensive instructions to guide LLMs on the preferred workflow:
- Use external GitHub MCP server tools for data fetching when available
- Use this server's specialized tools for AI model detection and bias-resistant review
- Fall back to this server's GitHub CLI tools only when external GitHub tools are unavailable

### Model Detection Strategy

The server detects AI models from commit co-authors:
- Parses `Co-Authored-By:` trailers in commit messages
- Recognizes patterns like `Co-Authored-By: Claude <noreply@anthropic.com>`
- Supports Claude, GPT/OpenAI, GitHub Copilot, Gemini, and other AI tools

### Cross-Model Review Process

1. **Model Exclusion**: The server tracks which model generated the code and attempts to request a different model family for review
2. **Client Sampling**: Uses MCP's sampling feature to request the client use a different model (success depends on client capabilities)
3. **Manual Fallback**: When sampling isn't supported, the tool provides ready-to-use prompts and specific model recommendations, instructing users to manually run the review with a different model family
4. **Structured Output**: Returns consistent review format with severity levels, metrics, and alternatives
5. **Flexible Model Selection**: Uses metadata and hints to guide client model choice, though actual model selection depends on client implementation

**Fallback for Non-Sampling Clients**: The server ensures bias-resistant evaluation regardless of client capabilities. When automatic sampling fails, users receive clear instructions and ready-to-use prompts for manual cross-model review, maintaining the bias mitigation approach even with basic MCP clients.

## Installation

```bash
# Clone the repository
git clone <repository-url>
cd mcp-code-crosscheck

# Install dependencies
npm install

# Build the project
npm run build
```

## Usage

### As an MCP Server

Add to your MCP client configuration:

```json
{
  "mcpServers": {
    "code-crosscheck": {
      "command": "node",
      "args": ["path/to/mcp-code-crosscheck/dist/index.js"],
      "env": {}
    }
  }
}
```

### Tool Usage

#### Modular Workflow Examples

**Preferred Workflow (with GitHub MCP server):**
```javascript
// 1. Use GitHub MCP server to fetch commit data
{
  "method": "tools/call",
  "params": {
    "name": "github_get_commit", // GitHub MCP server tool
    "arguments": {
      "owner": "modelcontextprotocol",
      "repo": "inspector", 
      "commit_sha": "fdae89ecbfec8fda5d166277ab77398e6d3c06c9"
    }
  }
}

// 2. Use this server to detect AI model from authors
{
  "method": "tools/call",
  "params": {
    "name": "detect_model_from_authors",
    "arguments": {
      "authors": [
        // Can handle any GitHub API format - LLM extracts what's needed
        {
          "login": "claude-ai",
          "id": 12345,
          "node_id": "MDQ6VXNlcjEyMzQ1",
          "avatar_url": "https://github.com/images/error/octocat_happy.gif",
          "type": "User",
          "name": "Claude",
          "email": "noreply@anthropic.com"
        }
      ]
    }
  }
}

// 3. Use this server for bias-resistant review
{
  "method": "tools/call",
  "params": {
    "name": "review_code",
    "arguments": {
      "code": "def process_user_input(data):\n    return eval(data)",
      "generationModel": "claude",
      "language": "python",
      "context": "User input processing function",
      "reviewType": "security"
    }
  }
}
```

**Fallback Workflow (without GitHub MCP server):**
```javascript
// 1. Use this server's GitHub CLI fallback
{
  "method": "tools/call",
  "params": {
    "name": "fetch_commit",
    "arguments": {
      "commitHash": "fdae89ecbfec8fda5d166277ab77398e6d3c06c9",
      "repo": "modelcontextprotocol/inspector"
    }
  }
}

// 2. Detect model from the fetched authors
{
  "method": "tools/call",
  "params": {
    "name": "detect_model_from_authors",
    "arguments": {
      "authors": [/* authors from step 1 */]
    }
  }
}

// 3. Review with detected model
{
  "method": "tools/call",
  "params": {
    "name": "review_code",
    "arguments": {
      "code": "def process_user_input(data):\n    return eval(data)",
      "generationModel": "claude",
      "language": "python",
      "reviewType": "security"
    }
  }
}
```

#### Individual Tool Reference

**review_code**
- `code` (required): The code to review
- `generationModel` (required): Model that generated the code
- `language` (optional): Programming language
- `context` (optional): Additional context about the code
- `reviewType` (optional): security, performance, maintainability, general

**detect_model_from_authors**
- `authors` (required): Array of author objects in any format - the tool will extract available information for AI model detection

**fetch_commit** (fallback tool)
- `commitHash` (required): Git commit hash to fetch
- `repo` (optional): GitHub repository (owner/repo format)

**fetch_pr_commits** (fallback tool)
- `prNumber` (required): GitHub PR number to fetch commits from
- `repo` (optional): GitHub repository (owner/repo format)

**Output Format:**
- Structured markdown with clear sections for summary, issues, metrics, and alternatives
- Quantitative ratings (1-3 scale) for error handling, performance, security, maintainability
- Metadata appended showing review model, generation model, and strategy used
- For bias-aware and hybrid strategies: bias triggers detection section

### Prompt Usage

Use the provided prompt for comprehensive code reviews:

```javascript
// Comprehensive code review
{
  "method": "prompts/get",
  "params": {
    "name": "code_review",
    "arguments": {
      "code": "your code here"
    }
  }
}
```

## Limitations and Considerations

### When This Approach May Not Help
- **Simple code**: For straightforward implementations, cross-model review may add complexity without benefit
- **Domain-specific code**: Models may lack specialized knowledge regardless of which one reviews
- **Subjective preferences**: Style and architectural decisions may vary between models without clear "better" choices

### Client Compatibility
- **Advanced MCP clients**: Full automatic cross-model evaluation via sampling API
- **Basic MCP clients**: Manual cross-model workflow with provided prompts and model recommendations
- **All clients**: Bias-resistant review is achievable regardless of sampling support

**Recommendation**: Consider this as one tool among many for code review, not a replacement for human judgment or comprehensive testing.

## Architecture

### Modular Design

```mermaid
graph TB
    subgraph "GitHub Data Sources"
        GH_MCP[GitHub MCP Server]
        GH_CLI[GitHub CLI Fallback]
    end
    
    subgraph "Code Crosscheck Server"
        DETECT[detect_model_from_authors]
        REVIEW[review_code]
        FETCH_COMMIT[fetch_commit]
        FETCH_PR[fetch_pr_commits]
    end
    
    subgraph "Client/LLM"
        CLIENT[MCP Client]
        LLM[Language Model]
    end
    
    CLIENT -->|Preferred| GH_MCP
    CLIENT -->|Fallback| FETCH_COMMIT
    CLIENT -->|Fallback| FETCH_PR
    
    GH_MCP -->|Author Data| DETECT
    FETCH_COMMIT -->|Author Data| DETECT
    FETCH_PR -->|Author Data| DETECT
    
    DETECT -->|Generation Model| REVIEW
    REVIEW -->|Cross-Model Request| CLIENT
    CLIENT -->|Different Model| LLM
    LLM -->|Review Results| CLIENT
```

### Workflow Sequence

```mermaid
sequenceDiagram
    participant Client
    participant GitHub_MCP as GitHub MCP Server
    participant Crosscheck as Code Crosscheck Server
    participant LLM as Review Model

    Note over Client: Preferred Workflow
    Client->>GitHub_MCP: fetch commit/PR data
    GitHub_MCP-->>Client: commit data + authors
    
    Client->>Crosscheck: detect_model_from_authors
    Crosscheck-->>Client: detected generation model
    
    Client->>Crosscheck: review_code (with generation model)
    Note over Crosscheck: Create model exclusion preferences
    Crosscheck->>Client: sampling request (exclude generation model)
    
    Client->>LLM: review request (different model)
    LLM-->>Client: review results
    Client-->>Crosscheck: review response
    
    Note over Crosscheck: Parse and validate results
    Crosscheck-->>Client: structured review output
```

## Related

- [Model Context Protocol](https://modelcontextprotocol.io/)
- [MCP TypeScript SDK](https://github.com/modelcontextprotocol/typescript-sdk)
